/*
 *  Copyright (c) 2019 International Characters.
 *  This software is licensed to the public under the Open Software License 3.0.
 *  icgrep is a trademark of International Characters.
 */

#include <kernel/streamutils/deletion.h>                      // for DeletionKernel
#include <kernel/io/source_kernel.h>
#include <kernel/basis/p2s_kernel.h>
#include <kernel/basis/s2p_kernel.h>                    // for S2PKernel
#include <kernel/io/stdout_kernel.h>                 // for StdOutKernel_
#include <kernel/streamutils/pdep_kernel.h>
#include <llvm/IR/Function.h>                      // for Function, Function...
#include <llvm/IR/Module.h>                        // for Module
#include <llvm/Support/CommandLine.h>              // for ParseCommandLineOp...
#include <llvm/Support/Debug.h>                    // for dbgs
#include <pablo/pablo_kernel.h>                    // for PabloKernel
#include <pablo/pablo_toolchain.h>
#include <pablo/parse/pablo_source_kernel.h>
#include <pablo/parse/pablo_parser.h>
#include <pablo/parse/simple_lexer.h>
#include <pablo/parse/rd_parser.h>
#include <re/adt/re_name.h>
#include <re/adt/re_re.h>
#include <grep/grep_kernel.h>
#include <re/cc/cc_compiler.h>
#include <re/cc/cc_compiler_target.h>
#include <re/unicode/resolve_properties.h>
#include <pablo/bixnum/bixnum.h>
#include <kernel/core/kernel_builder.h>
#include <pablo/pe_zeroes.h>
#include <toolchain/toolchain.h>
#include <kernel/pipeline/driver/cpudriver.h>
#include <kernel/core/streamset.h>
#include <kernel/scan/index_generator.h>
#include <kernel/scan/reader.h>
#include <kernel/streamutils/run_index.h>
#include <kernel/streamutils/stream_select.h>
#include <kernel/streamutils/streams_merge.h>
#include <kernel/util/bixhash.h>
#include <kernel/util/debug_display.h>
#include <llvm/ADT/StringRef.h>
#include <llvm/Support/raw_ostream.h>
#include <pablo/bixnum/bixnum.h>
#include <pablo/pe_zeroes.h>
#include <pablo/builder.hpp>
#include <pablo/pe_ones.h>
#include <unicode/utf/utf_compiler.h>
#include <re/unicode/resolve_properties.h>
#include <re/cc/cc_compiler.h>
#include <re/cc/cc_compiler_target.h>
#include <fcntl.h>
#include <iostream>
#include <iomanip>
#include <kernel/pipeline/pipeline_builder.h>
#include <boost/graph/adjacency_list.hpp>
#include <random>
#include <boost/filesystem.hpp>
#include <fileselect/file_select.h>
#include <boost/container/stable_vector.hpp>
#include "ga/algorithm.hpp"
#include <sys/stat.h>
#include <kernel/util/linebreak_kernel.h>
#include <pablo/bixnum/bixnum.h>
#include "murmur3/MurmurHash3.h"
#include <boost/intrusive/detail/math.hpp>
#include <llvm/ADT/BitVector.h>
#include <util/slab_allocator.h>

using boost::intrusive::detail::ceil_log2;

#define USE_BIX_SUB_HASH

#define GET_HASH_STATISTISTICS

using namespace pablo;
using namespace parse;
using namespace kernel;
using namespace llvm;
using namespace codegen;
using namespace boost;
using namespace boost::graph;
namespace fs = boost::filesystem;

static cl::OptionCategory HashDemoOptions("Hash Demo Options", "Hash demo options.");

static cl::list<std::string> inputFiles(cl::Positional, cl::desc("<input file ...>"), cl::OneOrMore, cl::cat(HashDemoOptions));

constexpr static size_t NumOfBasisBits = 8;

static cl::opt<unsigned> NumOfBuckets("buckets",  cl::init(128),
                                      cl::desc("number of hash buckets used for chi-squared test (default=128)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<unsigned> NumOfSteps("steps",  cl::init(4),
                                      cl::desc("number of steps used in the hashing function. Only the first 2^(STEPS) - 1 "
                                               "characters of each word will be represented in the hash code (default=4)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<unsigned> NumOfBitFlips("bitflips",  cl::init(4),
                                      cl::desc("number of bit flips generated for the hashing function data. "
                                               "Used to determine avalanche probability (default=64)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<unsigned> NumOfHashBits("hashwidth",  cl::init(32),
                                      cl::desc("number of hash bits generated by the hashing function (default=16)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<uint64_t> BaseSeed("seed",  cl::init(0),
                                      cl::desc("initial seed given to pseudorandom number generator"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));


static cl::opt<bool> Murmur3Only("murmur3only",  cl::init(false),
                                      cl::desc("only run murmur3 has on data"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

template<typename ValueType>
class HashArrayMappedTrie {

    struct DataNode {
        const char *    Start;
        size_t          Length;
        ValueType       Value;
        DataNode *      Next;
    };

public:

    std::pair<bool, ValueType> insert(const uint32_t hashKey, const char * start, const char * end, ValueType value) {

        ++TotalEntries;

        const auto mask = (1ULL << BitsPerKey) - 1ULL;
        void ** current = Root;
        auto key = hashKey;
        for (size_t i = 1; i < TrieLevels; ++i) {
           void ** node = current + (key & mask);
           if (*node == nullptr) {
               void ** newNode = mInternalAllocator.allocate<void *>(1ULL << BitsPerKey);
               for (unsigned i = 0; i < (1ULL << BitsPerKey); ++i) {
                   newNode[i] = nullptr;
               }
               *node = newNode;
           }
           key >>= BitsPerKey;
           current = node;
        }



        DataNode ** data = (DataNode **)current;

        DataNode * prior = nullptr;

        assert (*end == '\n');

        const size_t length = end - start;

        assert (length <= (1ULL << NumOfSteps) + 1ULL);

        for (;;) {

            if (LLVM_LIKELY(*data == nullptr)) {
                DataNode * newNode = mInternalAllocator.allocate<DataNode>(1);
                char * string = mInternalAllocator.allocate<char>(length);
                std::strncpy(string, start, length);
                newNode->Start = string;
                newNode->Length = length;
                newNode->Value = value;
                newNode->Next = prior;
                *((DataNode **)current) = newNode;
                ++NewEntries;
                return std::make_pair(true, value);
            }

            prior = *data;

            if (prior->Length == length) {
                if (std::strncmp(prior->Start, start, length) == 0) {
                    ++ExistingEntries;
                    return std::make_pair(false, prior->Value);
                }
            }

            data = &(prior->Next);

        }


    }

    HashArrayMappedTrie(const size_t BitsPerKey, const size_t TrieLevels)
    : Root(mInternalAllocator.allocate<void *>(1ULL << BitsPerKey))
    , BitsPerKey(BitsPerKey)
    , TrieLevels(TrieLevels) {
        for (unsigned i = 0; i < (1ULL << BitsPerKey); ++i) {
            Root[i] = nullptr;
        }
    }

    void clear() {
        mInternalAllocator.Reset();
        Root = mInternalAllocator.allocate<void *>(1ULL << BitsPerKey);
        for (unsigned i = 0; i < (1ULL << BitsPerKey); ++i) {
            Root[i] = nullptr;
        }
        NewEntries = 0;
        ExistingEntries = 0;
        TotalEntries = 0;
    }

    size_t getNewEntries() const {
        return NewEntries;
    }

    size_t getExistingEntries() const {
        assert (ExistingEntries == (TotalEntries - NewEntries));
        return ExistingEntries;
    }

private:

    SlabAllocator<char> mInternalAllocator;

    void ** Root;
    const size_t BitsPerKey;
    const size_t TrieLevels;

    size_t NewEntries = 0;
    size_t ExistingEntries = 0;
    size_t TotalEntries = 0;
};

static HashArrayMappedTrie<uint64_t> HT2(5, NumOfHashBits / 5);

class IdentifyLastSelector final: public pablo::PabloKernel {
public:
    IdentifyLastSelector(BuilderRef b, StreamSet * selector_span, StreamSet * selectors)
    : PabloKernel(b, "IdentifyLastSelector",
                  {Binding{"selector_span", selector_span, FixedRate(), LookAhead(1)}},
                  {Binding{"selectors", selectors}}) {}
protected:
    void generatePabloMethod() override;
};


void IdentifyLastSelector::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());
    // TODO: we shouldn't need this kernel to obtain the last mark of each run of a selector_span
    // if we can push them ahead to the end of the run but that may add an N advances where N is
    // the hash code bit width.
    pablo::Integer * pb_ZERO = pb.getInteger(0);
    PabloAST * span = pb.createExtract(getInputStreamVar("selector_span"), pb_ZERO);
    PabloAST * la = pb.createLookahead(span, 1);
    PabloAST * selectors = pb.createAnd(span, pb.createNot(la));
    pb.createAssign(pb.createExtract(getOutputStreamVar("selectors"), pb_ZERO), selectors);
}

class CountTwoStreams final: public pablo::PabloKernel {
public:
    CountTwoStreams(BuilderRef b, StreamSet * A, StreamSet * B)
    : PabloKernel(b, "CountTwoStreams",
    {Binding{"A", A}, Binding{"B", B}},
    {},
    {},
    {}) {
        addAttribute(SideEffecting());
    }
protected:
    void generatePabloMethod() override;
};


void CountTwoStreams::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());
    // TODO: we shouldn't need this kernel to obtain the last mark of each run of a selector_span
    // if we can push them ahead to the end of the run but that may add an N advances where N is
    // the hash code bit width.
    pablo::Integer * pb_ZERO = pb.getInteger(0);
    PabloAST * A = pb.createCount(pb.createExtract(getInputStreamVar("A"), pb_ZERO), "cA");
    PabloAST * B = pb.createCount(pb.createExtract(getInputStreamVar("B"), pb_ZERO), "cB");
    pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {A});
    pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {B});
}


// using BixHashGenome = adjacency_list<vecS, vecS, bidirectionalS>;

// using BixHashGenome = std::vector<BitVector>;

struct BixHashGenome {
    std::vector<BitVector> BasisSelectors;
    std::vector<std::vector<unsigned>> MixOrderings;
};

class BixHashGenerator final: public pablo::PabloKernel {
public:
    static std::string makeGenName(const BixHashGenome & genome) {
        std::string tmp;
        tmp.reserve(1024);
        raw_string_ostream out(tmp);
        out << "BHG:" << NumOfSteps << ":" << NumOfHashBits << " BASIS: ";

        for (unsigned i = 0; i < NumOfHashBits; ++i) {
            auto & B = genome.BasisSelectors[i];
            char joiner = '{';
            for (auto k : B.set_bits()) {
                out << joiner << k;
                joiner = ',';
            }
            out << "} ";
        }



        for (unsigned i = 0; i < NumOfSteps; ++i) {
            auto & B = genome.MixOrderings;
            out << "MIX_" << i << ": ";
            char joiner = '{';
            for (auto k : B[i]) {
                out << joiner << k;
                joiner = ',';
            }
            out << "} ";
        }

        out.flush();
        return tmp;
    }
public:
    BixHashGenerator(BuilderRef b, const BixHashGenome & genome, StreamSet * basis, StreamSet * LineFeeds, StreamSet * hashes, StreamSet * selector_span)
    : BixHashGenerator(b, genome, makeGenName(genome), basis, LineFeeds, hashes, selector_span) {

    }
    bool hasSignature() const override { return true; }
    llvm::StringRef getSignature() const override {
        return mSiganture;
    }
private:
    BixHashGenerator(BuilderRef b, const BixHashGenome & genome, std::string && signature, StreamSet * basis, StreamSet * runs, StreamSet * hashes, StreamSet * selector_span)
    : PabloKernel(b, "bhg" + getStringHash(signature),
                  {Binding{"basis", basis}, Binding{"runs", runs}},
                  {Binding{"hashes", hashes}, Binding{"selector_span", selector_span}})
    , mGenome(genome) {
        assert (hashes->getNumElements() == NumOfHashBits);
        assert (selector_span->getNumElements() == 1);
    }
protected:
    void generatePabloMethod() override;
private:
    const BixHashGenome & mGenome;
    const std::string mSiganture;
};

void BixHashGenerator::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());

    // TODO: if we assume this version of a BixHash is designed for UTF-8 text, are there optimal mixes?
    // Can we use a genetic algorithm to deduce it?

    std::vector<PabloAST *> basis = getInputStreamSet("basis");
    const size_t n = NumOfBasisBits; assert (n == basis.size());
    PabloAST * run = getInputStreamSet("runs")[0];
    const size_t m = NumOfHashBits;
    const size_t steps = NumOfSteps;

    std::vector<PabloAST *> hash(m * 2);

    // Let N be the number of basis bits (8), M be the number of hash bits and K be the number of steps.
    // The mHashMix starts with a set of N and a set of M vertices, denoted A and B. Edges are strictly
    // between vertices in A and B represent xor operations of the basis bits from A.

    PabloAST * const zeroes = pb.createZeroes();

    for (unsigned i = 0; i < m; ++i) {
        const auto & S = mGenome.BasisSelectors[i];
        int j = S.find_first();
        PabloAST * h = zeroes;
        if (j == -1) {
            h = zeroes;
        } else {
            assert (j < basis.size());
            h = basis[j];
            while ((j = S.find_next(j)) != -1) {
                assert (j < basis.size());
                h = pb.createXor(h, basis[j]);
            }
        }
        hash[i] = h;
    }

    // In each step, the select stream will mark positions that are
    // to receive bits from prior locations in the symbol.   The
    // select stream must ensure that no bits from outside the symbol
    // are included in the calculated hash value.
    PabloAST * select = run;

    PabloAST * carry = nullptr;

    for (unsigned i = 0; i < steps; ++i) {

        const auto shft = 1U << i;
     //   const auto s = n + m * i;

        const auto & mix = mGenome.MixOrderings[i];

        assert (mix.size() == m);

        const auto x = ((i & 1) == 0) ? 0 : m;
        const auto y = (m ^ x);

        for (unsigned j = 0; j < m; ++j) {

            PabloAST * A = hash[x + mix[j]]; assert (A);
            PabloAST * B = hash[x + mix[((j + 1) % m)]]; assert (B);
            B = pb.createAnd(select, pb.createAdvance(B, shft));

            PabloAST * val = nullptr;
            if (carry == nullptr) {
                val = pb.createXor(A, B);
                carry = pb.createAnd(pb.createNot(A), B);
            } else {
                val = pb.createXor3(A, B, carry);
                carry = pb.createMajority3(pb.createNot(A), B, carry);
            }
            hash[y + j] = val;
        }
        #ifndef NDEBUG
        for (unsigned j = 0; j < m; ++j) {
            hash[x + j] = nullptr;
        }
        #endif
        select = pb.createAnd(select, pb.createAdvance(select, shft));
    }

    const auto f = ((steps & 1) == 0) ? 0 : m;

    Var * hashVar = getOutputStreamVar("hashes");
    for (unsigned i = 0; i < m; i++) {
        PabloAST * const expr = hash[f + i]; assert (expr);
     //   pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {expr});
        pb.createAssign(pb.createExtract(hashVar, pb.getInteger(i)), expr);
    }

  //  pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {run});


    // if the value is still in the select span, we did not include it in the hash'ed value.
    PabloAST * const selectors = pb.createAnd(run, pb.createNot(select), "selectors");

 //   pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {selectors});

  //  pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {selectors});

    pb.createAssign(pb.createExtract(getOutputStreamVar("selector_span"), pb.getInteger(0)), selectors);

}


class NegateStreamSet : public pablo::PabloKernel {
public:
    NegateStreamSet(BuilderRef b, kernel::StreamSet * input, kernel::StreamSet * output);
protected:
    void generatePabloMethod() override;
};

NegateStreamSet::NegateStreamSet (BuilderRef b, kernel::StreamSet * input, kernel::StreamSet * output)
: PabloKernel(b, "NegateStreamSet" + std::to_string(input->getNumElements()),
          {Binding{"input", input}},
          {Binding{"output", output}}) {

}


void NegateStreamSet::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());
    const auto input = getInputStreamSet("input");
    for (unsigned i = 0; i < input.size(); ++i) {
        PabloAST * v = pb.createNot(input[i]);
        pb.createAssign(pb.createExtract(getOutputStreamVar("output"), pb.getInteger(i)), v);
    }
}


class HashTable {
public:

    HashTable()
    : mHashCodeIndex(0)
    , mNumOfSameBitValues(0)
    , mNumOfDifferentBitValues(0) {

    }

    void insert(const uint64_t hash_code, const char * const /* start */, const char * const /* end */) {

        // errs() << hash_code << "\n";

        const uint64_t hb = NumOfHashBits;
        assert (hash_code < (1ULL << hb));
        const uint64_t nb = NumOfBuckets;
        const uint64_t idx = (hash_code * nb) >> hb;
        assert (idx < nb);
        // Since we know that the data read filters out any duplicates. We don't bother to
        // store the actual strings and instead just trust that they're unique.
        mTable[idx]++;
        // By construction of the input data, we know that we will see the original word then
        // NumOfBitFlips copies with exactly 1 bit flipped.

        mBitFlipHistory[mHashCodeIndex] = hash_code;

        if (mHashCodeIndex == NumOfBitFlips) {
            uint64_t diffCount = 0;
            for (unsigned i = 1; i <= NumOfBitFlips; ++i) {
                const auto A = mBitFlipHistory[i];
                for (unsigned j = 0; j < i; ++j) {
                    const auto B = mBitFlipHistory[j];
                    // if items differ, C's bit will be 1.
                    diffCount += __builtin_popcountll(A ^ B);
                }
            }

            mNumOfDifferentBitValues += diffCount;
            const size_t total = ((NumOfBitFlips * NumOfBitFlips) * NumOfHashBits) / 2;
            mNumOfSameBitValues += (total - diffCount);

            mHashCodeIndex = 0;
        } else {
            ++mHashCodeIndex;
        }



    }

    void clear() {
        mTable.resize(NumOfBuckets);
        for (unsigned i = 0; i < NumOfBuckets; ++i) {
            mTable[i] = 0;
        }
        mBitFlipHistory.resize(NumOfBitFlips + 1);
       for (unsigned i = 0; i <= NumOfBitFlips; ++i) {
            mBitFlipHistory[i] = 0;
        }
        mHashCodeIndex = 0;
        mNumOfSameBitValues = 0;
        mNumOfDifferentBitValues = 0;
    }

    double calculateScore() const {
        const auto p = percentDifference();
        assert (0.0 <= p && p <= 2.0);
        const auto c = chiSquareTest();
        return c * std::pow(10.0, p);
    }

    double chiSquareTest() const {
        size_t totalCount = 0;
        for (const auto T : mTable) {
            totalCount += T;
        }
        if (LLVM_UNLIKELY(totalCount == 0)) {
            return std::numeric_limits<double>::quiet_NaN();
        }

        errs() << "SLOT,FREQ\n";

        const double expected = (double)(totalCount) / (double)(NumOfBuckets);
        double sum = 0.0;
        for (unsigned i = 0; i < NumOfBuckets; ++i) {
            const auto T = mTable[i];
            errs() << i << "," << T << "\n";
            const double x = (double)(T) - expected;
            // could optimize out but run the risk of the sum becoming too big
            sum += ((x * x) / expected);
        }
        errs() << "\n";
        return sum / (double)(NumOfBuckets);
    }

    double percentDifference() const {
        const size_t total = (mNumOfSameBitValues + mNumOfDifferentBitValues);
        if (LLVM_UNLIKELY(total == 0)) {
            return std::numeric_limits<double>::quiet_NaN();
        }
        size_t d = 0;
        if (mNumOfSameBitValues > mNumOfDifferentBitValues) {
            d = mNumOfSameBitValues - mNumOfDifferentBitValues;
        } else {
            d = mNumOfDifferentBitValues - mNumOfSameBitValues;
        }
        return ((double)2 * d) / ((double)total);
    }

private:
    std::vector<size_t>                 mTable;
    std::vector<uint64_t>               mBitFlipHistory;
    size_t                              mHashCodeIndex;

    size_t                              mNumOfSameBitValues;
    size_t                              mNumOfDifferentBitValues;

};

#ifdef GET_HASH_STATISTISTICS
static HashTable HT1;
#endif



typedef void (*HashDemoFunctionType)(const char * data, const size_t length);

extern "C" void hashtable_callback(uint64_t hashval, const char * const start, const char * const end) {
    assert (start < end);
    #ifdef GET_HASH_STATISTISTICS
    HT1.insert(hashval, start, end);
    #endif
    HT2.insert(hashval, start, end, 0);
}

extern "C" void hashtable_clear() {
    #ifdef GET_HASH_STATISTISTICS
    HT1.clear();
    #endif
    HT2.clear();
}


class PopulateHashTable : public MultiBlockKernel {
public:
    PopulateHashTable(BuilderRef b, StreamSet * const codeUnitStream, StreamSet * const SymbolEnds, StreamSet * const HashValues);

    void linkExternalMethods(BuilderRef b) override;
private:
    void generateMultiBlockLogic(BuilderRef b, llvm::Value * const numOfStrides) override;
};

PopulateHashTable::PopulateHashTable(BuilderRef b, StreamSet * const codeUnitStream, StreamSet * const SymbolEnds, StreamSet * const HashValues)
: MultiBlockKernel(b, "PopulateHashTable",
// inputs
{Binding{"codeUnitStream", codeUnitStream, FixedRate(), Deferred()} // <- not the right I/O def. should be the position of the symbol ends
, Binding{"SymbolEnds", SymbolEnds}
, Binding{"HashValues", HashValues, PopcountOf("SymbolEnds")}},
// outputs
{},
// input scalars
{},
// output scalars
{},
// kernel state
{}) {
     assert (SymbolEnds->getNumElements() == 1);
     assert (HashValues->getNumElements() == 1);
     assert (codeUnitStream->getNumElements() == 1);
     addAttribute(SideEffecting());
}

void PopulateHashTable::linkExternalMethods(BuilderRef b) {
    b->LinkFunction("hashtable_callback", hashtable_callback);
}

void PopulateHashTable::generateMultiBlockLogic(BuilderRef b, llvm::Value * const numOfStrides) {

    Constant * const sz_ZERO = b->getSize(0);
    Constant * const sz_ONE = b->getSize(1);
    IntegerType * sizeTy = b->getSizeTy();
    const auto sizeTyWidth = sizeTy->getBitWidth();

    Constant * const sz_BITS = b->getSize(sizeTyWidth);

    Constant * const sz_STRIDEWIDTH = b->getSize(mStride);

    assert (mStride == b->getBitBlockWidth());

    assert ((mStride % sizeTyWidth ) == 0);

    const auto vecsPerStride = mStride / sizeTyWidth;

    BasicBlock * const entryBlock = b->GetInsertBlock();

    // we expect that every block will have at least one marker

    BasicBlock * const stridePrologue = b->CreateBasicBlock("stridePrologue");
    BasicBlock * const strideCoordinateVecLoop = b->CreateBasicBlock("strideCoordinateVecLoop");
    BasicBlock * const strideCoordinateElemLoop = b->CreateBasicBlock("strideCoordinateElemLoop");
    BasicBlock * const strideCoordinateElemDone = b->CreateBasicBlock("strideCoordinateElemDone");
    BasicBlock * const strideCoordinateVecDone = b->CreateBasicBlock("strideCoordinateVecDone");
    BasicBlock * const strideCoordinatesDone = b->CreateBasicBlock("strideCoordinatesDone");

    Value * const initialProcessed = b->getProcessedItemCount("codeUnitStream");
    Value * const initialStrideCount = b->CreateUDiv(b->getProcessedItemCount("SymbolEnds"), sz_STRIDEWIDTH);
    Value * const hashProcessed = b->getProcessedItemCount("HashValues");

    b->CreateBr(stridePrologue);

    b->SetInsertPoint(stridePrologue);
    PHINode * const strideNumPhi = b->CreatePHI(sizeTy, 2);
    strideNumPhi->addIncoming(sz_ZERO, entryBlock);
    PHINode * const outerProcessedPhi = b->CreatePHI(sizeTy, 2);
    outerProcessedPhi->addIncoming(initialProcessed, entryBlock);
    PHINode * const outerHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    outerHashProcessedPhi->addIncoming(hashProcessed, entryBlock);

    Value * const symbolEnds = b->loadInputStreamBlock("SymbolEnds", sz_ZERO, strideNumPhi);
    Value * const strideBaseCharacterOffset = b->CreateMul(b->CreateAdd(initialStrideCount, strideNumPhi), sz_STRIDEWIDTH);
    FixedVectorType * const sizeVecTy = FixedVectorType::get(sizeTy, vecsPerStride);
    Value * const symbolEndVec = b->CreateBitCast(symbolEnds, sizeVecTy);

    b->CreateLikelyCondBr(b->bitblock_any(symbolEnds), strideCoordinateVecLoop, strideCoordinateVecDone);

    b->SetInsertPoint(strideCoordinateVecLoop);
    PHINode * const elemIdx = b->CreatePHI(sizeTy, 2, "elemIdx");
    elemIdx->addIncoming(sz_ZERO, stridePrologue);
    PHINode * const incomingProcessedPhi = b->CreatePHI(sizeTy, 2);
    incomingProcessedPhi->addIncoming(outerProcessedPhi, stridePrologue);
    PHINode * const incomingHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    incomingHashProcessedPhi->addIncoming(outerHashProcessedPhi, stridePrologue);

    Value * const elem = b->CreateExtractElement(symbolEndVec, elemIdx);
    b->CreateCondBr(b->CreateICmpNE(elem, sz_ZERO), strideCoordinateElemLoop, strideCoordinateElemDone);

    b->SetInsertPoint(strideCoordinateElemLoop);
    PHINode * const remainingPhi = b->CreatePHI(sizeTy, 2);
    remainingPhi->addIncoming(elem, strideCoordinateVecLoop);
    PHINode * const innerProcessedPhi = b->CreatePHI(sizeTy, 2);
    innerProcessedPhi->addIncoming(incomingProcessedPhi, strideCoordinateVecLoop);
    PHINode * const innerHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    innerHashProcessedPhi->addIncoming(incomingHashProcessedPhi, strideCoordinateVecLoop);

    Value * pos = b->CreateCountForwardZeroes(remainingPhi, "", true);
    pos = b->CreateAdd(pos, b->CreateMul(elemIdx, sz_BITS));
    pos = b->CreateAdd(pos, strideBaseCharacterOffset);

    FixedArray<Value *, 3> args;
    Value * const hashPtr = b->getRawInputPointer("HashValues", innerHashProcessedPhi);

    StreamSet * const ss = b->getInputStreamSet("HashValues");
    IntegerType * const hashValTy = b->getIntNTy(ss->getFieldWidth());
    args[0] = b->CreateZExt(b->CreateLoad(hashValTy, hashPtr), sizeTy);
    args[1] = b->getRawInputPointer("codeUnitStream", innerProcessedPhi);
    args[2] = b->getRawInputPointer("codeUnitStream", pos);

    b->CreateAssert (b->CreateICmpULT(innerProcessedPhi, pos), "string too small 1");

    b->CreateAssert (b->CreateICmpULT(args[1], args[2]), "string too small 2");

    Function * callbackFn = b->getModule()->getFunction("hashtable_callback"); assert (callbackFn);
    b->CreateCall(callbackFn->getFunctionType(), callbackFn, args);

    Value * const nextRemaining = b->CreateResetLowestBit(remainingPhi);
    remainingPhi->addIncoming(nextRemaining, strideCoordinateElemLoop);

    Value * const nextHashProcessed = b->CreateAdd(innerHashProcessedPhi, sz_ONE);
    innerHashProcessedPhi->addIncoming(nextHashProcessed, strideCoordinateElemLoop);

    Value * const nextStart = b->CreateAdd(pos, sz_ONE);
    innerProcessedPhi->addIncoming(nextStart, strideCoordinateElemLoop);

    b->CreateCondBr(b->CreateICmpNE(nextRemaining, sz_ZERO), strideCoordinateElemLoop, strideCoordinateElemDone);

    b->SetInsertPoint(strideCoordinateElemDone);

    PHINode * const nextProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextProcessedPhi->addIncoming(incomingProcessedPhi, strideCoordinateVecLoop);
    nextProcessedPhi->addIncoming(nextStart, strideCoordinateElemLoop);
    incomingProcessedPhi->addIncoming(nextProcessedPhi, strideCoordinateElemDone);

    PHINode * const nextHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextHashProcessedPhi->addIncoming(outerHashProcessedPhi, strideCoordinateVecLoop);
    nextHashProcessedPhi->addIncoming(nextHashProcessed, strideCoordinateElemLoop);
    incomingHashProcessedPhi->addIncoming(nextHashProcessedPhi, strideCoordinateElemDone);

    Value * const nextElemIdx = b->CreateAdd(elemIdx, sz_ONE);
    elemIdx->addIncoming(nextElemIdx, strideCoordinateElemDone);
    Value * const moreVecs = b->CreateICmpNE(nextElemIdx, b->getSize(vecsPerStride));
    b->CreateCondBr(moreVecs, strideCoordinateVecLoop, strideCoordinateVecDone);

    b->SetInsertPoint(strideCoordinateVecDone);
    PHINode * const nextOuterProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextOuterProcessedPhi->addIncoming(nextProcessedPhi, strideCoordinateElemDone);
    nextOuterProcessedPhi->addIncoming(outerProcessedPhi, stridePrologue);
    outerProcessedPhi->addIncoming(nextOuterProcessedPhi, strideCoordinateVecDone);

    PHINode * const nextOuterHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextOuterHashProcessedPhi->addIncoming(nextHashProcessedPhi, strideCoordinateElemDone);
    nextOuterHashProcessedPhi->addIncoming(outerHashProcessedPhi, stridePrologue);
    outerHashProcessedPhi->addIncoming(nextOuterHashProcessedPhi, strideCoordinateVecDone);

    Value * const nextStrideNum = b->CreateAdd(strideNumPhi, sz_ONE);
    strideNumPhi->addIncoming(nextStrideNum, strideCoordinateVecDone);

    b->CreateCondBr(b->CreateICmpULT(nextStrideNum, numOfStrides), stridePrologue, strideCoordinatesDone);

    b->SetInsertPoint(strideCoordinatesDone);
    b->setProcessedItemCount("codeUnitStream", nextOuterProcessedPhi);
}

#ifndef USE_BIX_SUB_HASH
HashDemoFunctionType hashdemo_gen (CPUDriver & driver, const BixHashGenome & genome) {
#else
HashDemoFunctionType hashdemo_gen (CPUDriver & driver) {
#endif
    auto & b = driver.getBuilder();
    auto P = driver.makePipeline({Binding{b->getInt8PtrTy(), "input"}, Binding{b->getSizeTy(), "length"}}, {});

    std::string tmp;
    raw_string_ostream nm(tmp);
    nm << "hashtester" << NumOfBasisBits << ":" << NumOfHashBits;
    P->setUniqueName(nm.str());

    Scalar * input = P->getInputScalar("input");
    Scalar * length = P->getInputScalar("length");

    // Source data
    StreamSet * const codeUnitStream = P->CreateStreamSet(1, NumOfBasisBits);
    P->CreateKernelCall<MemorySourceKernel>(input, length, codeUnitStream);

    StreamSet * const u8basis = P->CreateStreamSet(NumOfBasisBits);
    P->CreateKernelCall<S2PKernel>(codeUnitStream, u8basis);

    StreamSet * LineFeeds = P->CreateStreamSet(1);
    P->CreateKernelCall<LineFeedKernelBuilder>(u8basis, LineFeeds);

    StreamSet * Runs = P->CreateStreamSet(1);
    P->CreateKernelCall<NegateStreamSet>(LineFeeds, Runs);

    StreamSet * const BixHashes = P->CreateStreamSet(NumOfHashBits);
    StreamSet * SelectorSpans = P->CreateStreamSet(1);
#ifndef USE_BIX_SUB_HASH
    P->CreateKernelFamilyCall<BixHashGenerator>(genome, u8basis, Runs, BixHashes, SelectorSpans);
#else
    P->CreateKernelFamilyCall<BixSubHash>(u8basis, Runs, BixHashes, SelectorSpans, 4, BaseSeed);
#endif

    StreamSet * SymbolEnds = P->CreateStreamSet(1);
    P->CreateKernelCall<IdentifyLastSelector>(SelectorSpans, SymbolEnds);

    StreamSet * const compressed = P->CreateStreamSet(NumOfHashBits);

    P->CreateKernelCall<FieldCompressKernel>(Select(SymbolEnds, {0}), SelectOperationList{Select(BixHashes, streamutils::Range(0, NumOfHashBits))}, compressed, 64);

    StreamSet * const Hashes = P->CreateStreamSet(NumOfHashBits);

    P->CreateKernelCall<StreamCompressKernel>(SymbolEnds, compressed, Hashes, 64);

    StreamSet * const HashValues = P->CreateStreamSet(1, 1UL << ceil_log2(NumOfHashBits));

    if (NumOfHashBits <= 8) {
        P->CreateKernelCall<P2SKernel>(Hashes, HashValues);
    } else if (NumOfHashBits <= 16) {
        P->CreateKernelCall<P2S16Kernel>(Hashes, HashValues);
    } else {
        P->CreateKernelCall<P2S32Kernel>(Hashes, HashValues);
    }

    P->CreateKernelCall<PopulateHashTable>(codeUnitStream, LineFeeds, HashValues);

//    P->CreateKernelCall<CountTwoStreams>(LineFeeds, SymbolEnds);

    auto f = reinterpret_cast<HashDemoFunctionType>(P->compile());
    assert (f);
    return f;
}

class bixhash_optimization_problem {
public:
    using individual_type = std::shared_ptr<BixHashGenome>;
    using generator_type = std::mt19937;
    using fitness_type = double;


    constexpr static double recombination_rate = 0.4;

    constexpr static double mutation_rate = 0.02;

    auto evaluate(const individual_type & x, generator_type&) const -> fitness_type {
#ifndef USE_BIX_SUB_HASH
        CPUDriver pxDriver("hashtester");
        auto func = hashdemo_gen(pxDriver, *x);
        HT.clear();
        func(InputData.data(), InputData.size());
#endif
        const auto r = HT1.calculateScore();
        return r;
    }

    inline static auto nextLayerStart(const size_t u) -> size_t {
        if (u < NumOfBasisBits) {
            return NumOfHashBits;
        } else {
            assert (u < (NumOfBasisBits + (NumOfHashBits * NumOfSteps)));
            const auto m = (u - NumOfBasisBits) / NumOfHashBits;
            return (m + 1) * (NumOfHashBits) + NumOfBasisBits;
        }
    };

    inline static auto currentLayerStart(const size_t v) -> size_t {
        assert (v >= NumOfBasisBits);
        const auto m = (v - NumOfBasisBits) / NumOfHashBits;
        return m * (NumOfHashBits) + NumOfBasisBits;
    };

    auto recombine(const individual_type & x, const individual_type & y,
                  generator_type& rng) const -> std::array<individual_type, 2u> {

        if (!ga::draw(recombination_rate, rng)) {
            return std::array<individual_type, 2u> {x, y};
        }

        const BixHashGenome & A = *x;
        const BixHashGenome & B = *y;

        auto C = std::make_shared<BixHashGenome>();
        auto D = std::make_shared<BixHashGenome>();

        C->BasisSelectors.resize(NumOfHashBits);
        D->BasisSelectors.resize(NumOfHashBits);

        for (unsigned i = 0; i < NumOfHashBits; ++i) {
            C->BasisSelectors[i].resize(NumOfBasisBits, false);
            D->BasisSelectors[i].resize(NumOfBasisBits, false);
        }

        std::uniform_int_distribution<size_t> pos(1, NumOfHashBits * NumOfBasisBits - 2);
        const auto k = pos(rng);

        const auto toCopy = (k / NumOfHashBits);

        for (unsigned i = 0; i < toCopy; ++i) {
            C->BasisSelectors[i] = A.BasisSelectors[i];
            D->BasisSelectors[i] = B.BasisSelectors[i];
        }

        const auto p = k - (toCopy * NumOfHashBits);

        const auto & a = A.BasisSelectors[toCopy];
        const auto & b = B.BasisSelectors[toCopy];

        unsigned ai = a.find_first();
        while (ai < p) {
            assert (ai != -1U);
            C->BasisSelectors[toCopy].set(ai);
            ai = a.find_next(ai);
        }
        while (ai != -1U) {
            D->BasisSelectors[toCopy].set(ai);
            ai = a.find_next(ai);
        }

        unsigned bi = b.find_first();
        while (bi < p) {
            assert (bi != -1U);
            D->BasisSelectors[toCopy].set(bi);
            bi = b.find_next(bi);
        }
        while (bi != -1U) {
            C->BasisSelectors[toCopy].set(bi);
            bi = b.find_next(bi);
        }

        for (unsigned i = toCopy + 1; i < NumOfHashBits; ++i) {
            C->BasisSelectors[i] = B.BasisSelectors[i];
            D->BasisSelectors[i] = A.BasisSelectors[i];
        }

        std::uniform_int_distribution<size_t> pos2(1, NumOfHashBits * NumOfSteps - 2);
        const auto k2 = pos2(rng);

        const auto toCopy2 = (k2 / NumOfHashBits);

        assert (toCopy2 < NumOfSteps);

        C->MixOrderings.resize(NumOfSteps);
        D->MixOrderings.resize(NumOfSteps);

        for (unsigned i = 0; i < toCopy2; ++i) {
            C->MixOrderings[i] = A.MixOrderings[i];
            D->MixOrderings[i] = B.MixOrderings[i];
        }

        const auto p2 = k2 - (toCopy2 * NumOfHashBits);

        BitVector observed(NumOfHashBits, false);

        auto copy_selected = [&](const std::vector<unsigned> & A, const std::vector<unsigned> & B, std::vector<unsigned> & output) {

            assert (output.size() == 0);

            output.reserve(NumOfHashBits);

            for (unsigned i = 0; i < p; ++i) {
                const auto a = A[i];
                observed.set(a);
                output.push_back(a);
            }

            for (unsigned i = 0; i < NumOfHashBits; ++i) {
                const auto b = B[i];
                if (!observed.test(b)) {
                    output.push_back(b);
                }
            }

            assert (output.size() == NumOfHashBits);

        };

        copy_selected(A.MixOrderings[toCopy2], B.MixOrderings[toCopy2], C->MixOrderings[toCopy2]);

        observed.reset();

        copy_selected(B.MixOrderings[toCopy2], A.MixOrderings[toCopy2], D->MixOrderings[toCopy2]);

        for (unsigned i = toCopy2 + 1; i < NumOfSteps; ++i) {
            C->MixOrderings[i] = A.MixOrderings[i];
            D->MixOrderings[i] = B.MixOrderings[i];
        }

        return std::array<individual_type, 2u> {std::move(C), std::move(D)};

    }

    auto mutate(individual_type & x, generator_type& g) const -> void {

        auto & B = x->BasisSelectors;

        std::binomial_distribution<size_t> H(NumOfBasisBits, 1.0 / (double)(NumOfBasisBits));

        std::uniform_int_distribution<size_t> U(0, NumOfBasisBits - 1);

        for (unsigned i = 0; i < NumOfHashBits; ++i) {
            auto & bb = B[i];
            const auto k = H(g);
            for (unsigned j = 0; j < k; ++j) {
                bb.flip(U(g));
            }
        }

        auto & M = x->MixOrderings;

        std::binomial_distribution<size_t> S(NumOfHashBits / 2, 1.0 / (double)(NumOfHashBits));

        std::uniform_int_distribution<size_t> V(0, NumOfHashBits - 1);

        for (unsigned i = 0; i < NumOfSteps; ++i) {
            auto & mm = M[i];
            const auto k = S(g);
            for (unsigned j = 0; j < k; ++j) {
                const auto a = V(g);
                const auto b = V(g);
                if (a != b) {
                    std::swap(mm[a], mm[b]);
                }
            }
        }



    }

    bixhash_optimization_problem(std::vector<char> && data)
    : InputData(std::move(data)) {

    }

    static std::vector<individual_type> generate(const size_t count, generator_type & rng) {

        constexpr size_t n = NumOfBasisBits;
        const size_t m = NumOfHashBits;
        const size_t steps = NumOfSteps;

        const auto size = n + m * steps;

        std::vector<individual_type> P;
        P.reserve(count);

        std::vector<unsigned> selected(m);
        std::iota(selected.begin(), selected.end(), 0);

        for (unsigned p = 0; p < count; ++p) {

            auto C = std::make_shared<BixHashGenome>();

            auto & variables = C->BasisSelectors;

            variables.resize(m);

            for (unsigned i = 0; i < m; ++i) {
                assert (variables[i].size() == 0);
                variables[i].resize(n, false);
                assert (variables[i].empty());
            }

            std::binomial_distribution<size_t> M(m, 0.25);

            auto selected_pos = selected.end();



            for (unsigned i = 0; i < n; ++i) {
                if (selected_pos == selected.end()) {
                    std::shuffle(selected.begin(), selected.end(), rng);
                    selected_pos = selected.begin();
                }
                const auto k = M(rng) + 1U;
                for (unsigned j = 0; j != k; ++j) {
                    variables[*selected_pos++].set(i);
                }
            }

            auto & orderings = C->MixOrderings;

            orderings.resize(steps);

            for (unsigned i = 0; i < steps; ++i) {
                auto & O = orderings[i];
                O.resize(m);
                std::iota(O.begin(), O.end(), 0);
                std::shuffle(O.begin(), O.end(), rng);
            }

            P.emplace_back(std::move(C));
        }

        assert (P.size() == count);

        return P;
    }

private:

    const std::vector<char> InputData;

};

std::vector<char> readFileData(const std::vector<fs::path> & fileNames, bixhash_optimization_problem::generator_type & rng) {


    char * line = nullptr;
    size_t len = 0;

    errs() << " -- loading data\n";

    size_t requiredSize = 0;

    for (const fs::path & fileName: fileNames) {
        struct stat st;
        if (LLVM_LIKELY(stat(fileName.c_str(), &st) == 0)) {
            requiredSize += st.st_size;
        } else {
            report_fatal_error(StringRef{"cannot determine size of " + fileName.string()});
        }
    }

    std::vector<char> data(requiredSize * (NumOfBitFlips + 1));

    DenseSet<StringRef> observed;

    DenseSet<StringRef> temp;

    size_t current = 0;


    for (const fs::path & fileName: fileNames) {

        FILE * const fp = fopen(fileName.c_str(), "r");
        if (LLVM_UNLIKELY(fp == nullptr)) {
            report_fatal_error(StringRef{"cannot open " + fileName.string() + "."});
        }

        for (;;) {
            ssize_t r = getdelim(&line, &len, '\n', fp);
            // r is -1 or the number of characters read, including the delimiter
            if (LLVM_UNLIKELY(r == -1)) {
                break;
            }

            assert (r > 0);

            if (LLVM_LIKELY(line[r - 1] == '\n')) {
                --r;
            }

            // To simplify the process, we just discard any entries that aren't long
            // enough for bit flipping
            if (LLVM_UNLIKELY((r * 8) < NumOfBitFlips)) {
                continue;
            }

            const auto m = std::min<size_t>(r, 1ULL << NumOfSteps);

            assert (m < r || line[r] == '\n');

            line[m] = '\n';

            assert (m > 0);

            assert (data.size() >= (current + (m + 1) * (NumOfBitFlips + 1)));

            std::memcpy(&data[current], line, m + 1);

            auto p = current + m + 1;

            #ifndef NDEBUG
            for (auto i = current; i < (p - 1); ++i) {
                assert (data[i] != '\n');
            }
            #endif

            // we want only unique keys in the list to avoid needing sub-hashtables
            // for unique keys later.
            auto f = observed.find(StringRef{&data[current], m});
            if (LLVM_UNLIKELY(f != observed.end())) {
                continue;
            }

            assert (temp.empty());
            temp.insert(StringRef{&data[current], m});

            size_t remaining = 20;

            for (unsigned i = 0; i < NumOfBitFlips; ) {

                std::memcpy(&data[p], &data[current], m + 1);

                std::uniform_int_distribution<size_t> pos(0, (m * 8) - 1);

                const auto j = pos(rng);
                assert ((j / 8) < m);

                char & c = data[p + (j / 8)];

                c ^= (1U << (j & 7U));

                if (LLVM_UNLIKELY(c == '\n' || temp.count(StringRef{&data[p], m}) > 0 || observed.count(StringRef{&data[p], m}) > 0)) {
                    if (--remaining) {
                        goto reject;
                    } else {
                        continue;
                    }
                }

                temp.insert(StringRef{&data[p], m});

                p += (m + 1);
                ++i;
            }

            current = p;

            for (auto & t : temp) {
                observed.insert(t);
            }
reject:
            temp.clear();
        }

        fclose(fp);
    }
    free(line);

    data.resize(current);

    errs() << " -- loaded data: observed=" << observed.size() << "\n";

    return data;
}

void runMurmur3HashOnData(const std::vector<char> & data) {

    hashtable_clear();

    size_t start = 0;
    const auto len = data.size();

    const size_t mask = (1ULL << ((size_t)NumOfHashBits)) - 1ULL;

    for (size_t i = 0; i < len; ++i) {
        if (data[i] == '\n') {
            assert (start < i);
            uint32_t hashOut = 0;
            assert ((i - start) <= (1ULL << NumOfSteps) + 1ULL);
            MurmurHash3_x86_32(&data[start], i - start, 0, &hashOut);
            #ifdef GET_HASH_STATISTISTICS
            HT1.insert(hashOut & mask, &data[start], &data[i]);
            #endif
            HT2.insert(hashOut & mask, &data[start], &data[i], 0);
            start = i + 1;
        }
    }
    #ifdef GET_HASH_STATISTISTICS
    errs() << "MURMUR3 SCORE: CHI=" << HT1.chiSquareTest() << " % DIFF=" << HT1.percentDifference() << "\n";
    #endif
    errs() << "MURMUR3: NEW=" << HT2.getNewEntries() << " EXISTING=" << HT2.getExistingEntries() << "\n";

}

void runSubHashHashOnData(const std::vector<char> & data) {
#ifdef USE_BIX_SUB_HASH

    CPUDriver driver("ht");
    auto func = hashdemo_gen(driver);
    hashtable_clear();
    func(data.data(), data.size());
    #ifdef GET_HASH_STATISTISTICS
    errs() << "BIXHASH: CHI=" << HT1.chiSquareTest() << " % DIFF=" << HT1.percentDifference() << "\n";
    #endif
    errs() << "BIXHASH: NEW=" << HT2.getNewEntries() << " EXISTING=" << HT2.getExistingEntries() << "\n";
#endif
}

using rng = bixhash_optimization_problem::generator_type;

std::vector<char> getAllFileData(rng & generator) {
    CPUDriver pxDriver("allfiles");
    auto allFiles = argv::getFullFileList(pxDriver, inputFiles);
    return readFileData(allFiles, generator);
}

int main(int argc, char *argv[]) {
    codegen::ParseCommandLineOptions(argc, argv, {&HashDemoOptions, pablo_toolchain_flags(), codegen::codegen_flags()});

    uint64_t seed = 0;
    if (BaseSeed.isDefaultOption()) {
        std::random_device rng;
        seed = rng();
    } else {
        seed = BaseSeed;
    }

    bixhash_optimization_problem::generator_type generator(seed);

    auto fileData = getAllFileData(generator);

    runMurmur3HashOnData(fileData);

    if (Murmur3Only) {
        return 0;
    }

    runSubHashHashOnData(fileData);

    #ifndef USE_BIX_SUB_HASH

    bixhash_optimization_problem P(std::move(fileData));

    std::size_t elite_count = 0;
    const unsigned generation_count = 1;
    const unsigned population_count = 1;



    errs() << "INIT POP:\n";

    auto initial_population = bixhash_optimization_problem::generate(population_count, generator);

    errs() << "INIT ALGO:\n";

     HT.clear();

    ga::algorithm<bixhash_optimization_problem> algorithm(std::move(P), std::move(initial_population), elite_count, std::move(generator));

    // ===== Iterate the algorithm ===== //

    for (unsigned i = 0u; i < generation_count; ++i) {
        errs() << "ROUND: " << i << "\n";
        algorithm.iterate();
        const auto & solution = algorithm.population().front();
        errs() << " -- current fitness: " << solution.fitness << "\n";
    }
    // ===== Retrieve solution information ===== //

    // Every candidate solution in the population sorted by fitness.
    auto & solution = algorithm.population().front();

    errs() << BixHashGenerator::makeGenName(*solution.x);

   // printGraph(*(solution.x), errs(), "SOLUTION");
    errs() << "\n\nFITNESS: " << solution.fitness << "\n";
    #endif

    // LLVM seems to have an issue correctly exiting here. Its internal hash table gets stuck in an infinite loop?
    exit(0);
    return 0;
}
