/*
 *  Copyright (c) 2019 International Characters.
 *  This software is licensed to the public under the Open Software License 3.0.
 *  icgrep is a trademark of International Characters.
 */

#include <kernel/streamutils/deletion.h>                      // for DeletionKernel
#include <kernel/io/source_kernel.h>
#include <kernel/basis/p2s_kernel.h>
#include <kernel/basis/s2p_kernel.h>                    // for S2PKernel
#include <kernel/io/stdout_kernel.h>                 // for StdOutKernel_
#include <kernel/streamutils/pdep_kernel.h>
#include <llvm/IR/Function.h>                      // for Function, Function...
#include <llvm/IR/Module.h>                        // for Module
#include <llvm/Support/CommandLine.h>              // for ParseCommandLineOp...
#include <llvm/Support/Debug.h>                    // for dbgs
#include <pablo/pablo_kernel.h>                    // for PabloKernel
#include <pablo/pablo_toolchain.h>
#include <pablo/parse/pablo_source_kernel.h>
#include <pablo/parse/pablo_parser.h>
#include <pablo/parse/simple_lexer.h>
#include <pablo/parse/rd_parser.h>
#include <re/adt/re_name.h>
#include <re/adt/re_re.h>
#include <grep/grep_kernel.h>
#include <re/cc/cc_compiler.h>
#include <re/cc/cc_compiler_target.h>
#include <re/unicode/resolve_properties.h>
#include <pablo/bixnum/bixnum.h>
#include <kernel/core/kernel_builder.h>
#include <pablo/pe_zeroes.h>
#include <toolchain/toolchain.h>
#include <kernel/pipeline/driver/cpudriver.h>
#include <kernel/core/streamset.h>
#include <kernel/scan/index_generator.h>
#include <kernel/scan/reader.h>
#include <kernel/streamutils/run_index.h>
#include <kernel/streamutils/stream_select.h>
#include <kernel/streamutils/streams_merge.h>
#include <kernel/util/bixhash.h>
#include <kernel/util/debug_display.h>
#include <llvm/ADT/StringRef.h>
#include <llvm/Support/raw_ostream.h>
#include <pablo/bixnum/bixnum.h>
#include <pablo/pe_zeroes.h>
#include <pablo/builder.hpp>
#include <pablo/pe_ones.h>
#include <unicode/utf/utf_compiler.h>
#include <re/unicode/resolve_properties.h>
#include <re/cc/cc_compiler.h>
#include <re/cc/cc_compiler_target.h>
#include <fcntl.h>
#include <iostream>
#include <iomanip>
#include <kernel/pipeline/pipeline_builder.h>
#include <boost/graph/adjacency_list.hpp>
#include <random>
#include <boost/filesystem.hpp>
#include <fileselect/file_select.h>
#include <boost/container/stable_vector.hpp>
#include "ga/algorithm.hpp"
#include <sys/stat.h>
#include <kernel/util/linebreak_kernel.h>
#include <pablo/bixnum/bixnum.h>
#include "murmur3/MurmurHash3.h"
#include <boost/intrusive/detail/math.hpp>

using boost::intrusive::detail::ceil_log2;

#define USE_BIX_SUB_HASH

using namespace pablo;
using namespace parse;
using namespace kernel;
using namespace llvm;
using namespace codegen;
using namespace boost;
using namespace boost::graph;
namespace fs = boost::filesystem;

static cl::OptionCategory HashDemoOptions("Hash Demo Options", "Hash demo options.");

static cl::list<std::string> inputFiles(cl::Positional, cl::desc("<input file ...>"), cl::OneOrMore, cl::cat(HashDemoOptions));

constexpr static size_t NumOfBasisBits = 8;

static cl::opt<unsigned> NumOfBuckets("buckets",  cl::init(128),
                                      cl::desc("number of hash buckets used for chi-squared test (default=128)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<unsigned> NumOfSteps("steps",  cl::init(4),
                                      cl::desc("number of steps used in the hashing function. Only the first 2^(STEPS) - 1 "
                                               "characters of each word will be represented in the hash code (default=4)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<unsigned> NumOfBitFlips("bitflips",  cl::init(4),
                                      cl::desc("number of bit flips generated for the hashing function data. "
                                               "Used to determine avalanche probability (default=64)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<unsigned> NumOfHashBits("hashwidth",  cl::init(32),
                                      cl::desc("number of hash bits generated by the hashing function (default=16)"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));

static cl::opt<uint64_t> BaseSeed("seed",  cl::init(0),
                                      cl::desc("initial seed given to pseudorandom number generator"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));


static cl::opt<bool> Murmur3Only("murmur3only",  cl::init(false),
                                      cl::desc("only run murmur3 has on data"),
                                      cl::value_desc("positive integer"), cl::cat(HashDemoOptions));


/** ------------------------------------------------------------------------------------------------------------- *
 * @brief printGraph
 ** ------------------------------------------------------------------------------------------------------------- */
template <typename Graph>
void printGraph(const Graph & G, raw_ostream & out, const StringRef name = "G") {

//    out << "digraph \"" << name << "\" {\n";
//    for (auto v : make_iterator_range(vertices(G))) {
//        out << "v" << v << " [label=\"" << v << "\"];\n";
//    }
//    for (auto e : make_iterator_range(edges(G))) {
//        const auto s = source(e, G);
//        const auto t = target(e, G);
//        out << "v" << s << " -> v" << t << ";\n";
//    }

//    out << "}\n\n";
//    out.flush();
}

class IdentifyLastSelector final: public pablo::PabloKernel {
public:
    IdentifyLastSelector(BuilderRef b, StreamSet * selector_span, StreamSet * selectors)
    : PabloKernel(b, "IdentifyLastSelector",
                  {Binding{"selector_span", selector_span, FixedRate(), LookAhead(1)}},
                  {Binding{"selectors", selectors}}) {}
protected:
    void generatePabloMethod() override;
};


void IdentifyLastSelector::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());
    // TODO: we shouldn't need this kernel to obtain the last mark of each run of a selector_span
    // if we can push them ahead to the end of the run but that may add an N advances where N is
    // the hash code bit width.
    pablo::Integer * pb_ZERO = pb.getInteger(0);
    PabloAST * span = pb.createExtract(getInputStreamVar("selector_span"), pb_ZERO);
    PabloAST * la = pb.createLookahead(span, 1);
    PabloAST * selectors = pb.createAnd(span, pb.createNot(la));
    pb.createAssign(pb.createExtract(getOutputStreamVar("selectors"), pb_ZERO), selectors);
}

// using BixHashGenome = adjacency_list<vecS, vecS, bidirectionalS>;

// using BixHashGenome = std::vector<BitVector>;

struct BixHashGenome {
    std::vector<BitVector> BasisSelectors;
    std::vector<std::vector<unsigned>> MixOrderings;
};

class BixHashGenerator final: public pablo::PabloKernel {
public:
    static std::string makeGenName(const BixHashGenome & genome) {
        std::string tmp;
        tmp.reserve(1024);
        raw_string_ostream out(tmp);
        out << "BHG:" << NumOfSteps << ":" << NumOfHashBits << " BASIS: ";

        for (unsigned i = 0; i < NumOfHashBits; ++i) {
            auto & B = genome.BasisSelectors[i];
            char joiner = '{';
            for (auto k : B.set_bits()) {
                out << joiner << k;
                joiner = ',';
            }
            out << "} ";
        }



        for (unsigned i = 0; i < NumOfSteps; ++i) {
            auto & B = genome.MixOrderings;
            out << "MIX_" << i << ": ";
            char joiner = '{';
            for (auto k : B[i]) {
                out << joiner << k;
                joiner = ',';
            }
            out << "} ";
        }

        out.flush();
        return tmp;
    }
public:
    BixHashGenerator(BuilderRef b, const BixHashGenome & genome, StreamSet * basis, StreamSet * LineFeeds, StreamSet * hashes, StreamSet * selector_span)
    : BixHashGenerator(b, genome, makeGenName(genome), basis, LineFeeds, hashes, selector_span) {

    }
    bool hasSignature() const override { return true; }
    llvm::StringRef getSignature() const override {
        return mSiganture;
    }
private:
    BixHashGenerator(BuilderRef b, const BixHashGenome & genome, std::string && signature, StreamSet * basis, StreamSet * runs, StreamSet * hashes, StreamSet * selector_span)
    : PabloKernel(b, "bhg" + getStringHash(signature),
                  {Binding{"basis", basis}, Binding{"runs", runs}},
                  {Binding{"hashes", hashes}, Binding{"selector_span", selector_span}})
    , mGenome(genome) {
        assert (hashes->getNumElements() == NumOfHashBits);
        assert (selector_span->getNumElements() == 1);
    }
protected:
    void generatePabloMethod() override;
private:
    const BixHashGenome & mGenome;
    const std::string mSiganture;
};

void BixHashGenerator::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());

    // TODO: if we assume this version of a BixHash is designed for UTF-8 text, are there optimal mixes?
    // Can we use a genetic algorithm to deduce it?

    std::vector<PabloAST *> basis = getInputStreamSet("basis");
    const size_t n = NumOfBasisBits; assert (n == basis.size());
    PabloAST * run = getInputStreamSet("runs")[0];
    const size_t m = NumOfHashBits;
    const size_t steps = NumOfSteps;

    std::vector<PabloAST *> hash(m * 2);

    // Let N be the number of basis bits (8), M be the number of hash bits and K be the number of steps.
    // The mHashMix starts with a set of N and a set of M vertices, denoted A and B. Edges are strictly
    // between vertices in A and B represent xor operations of the basis bits from A.

    PabloAST * const zeroes = pb.createZeroes();

    for (unsigned i = 0; i < m; ++i) {
        const auto & S = mGenome.BasisSelectors[i];
        int j = S.find_first();
        PabloAST * h = zeroes;
        if (j == -1) {
            h = zeroes;
        } else {
            assert (j < basis.size());
            h = basis[j];
            while ((j = S.find_next(j)) != -1) {
                assert (j < basis.size());
                h = pb.createXor(h, basis[j]);
            }
        }
        hash[i] = h;
    }

    // In each step, the select stream will mark positions that are
    // to receive bits from prior locations in the symbol.   The
    // select stream must ensure that no bits from outside the symbol
    // are included in the calculated hash value.
    PabloAST * select = run;

    PabloAST * carry = nullptr;

    for (unsigned i = 0; i < steps; ++i) {

        const auto shft = 1U << i;
     //   const auto s = n + m * i;

        const auto & mix = mGenome.MixOrderings[i];

        assert (mix.size() == m);

        const auto x = ((i & 1) == 0) ? 0 : m;
        const auto y = (m ^ x);

        for (unsigned j = 0; j < m; ++j) {

            PabloAST * A = hash[x + mix[j]]; assert (A);
            PabloAST * B = hash[x + mix[((j + 1) % m)]]; assert (B);
            B = pb.createAnd(select, pb.createAdvance(B, shft));

            PabloAST * val = nullptr;
            if (carry == nullptr) {
                val = pb.createXor(A, B);
                carry = pb.createAnd(pb.createNot(A), B);
            } else {
                val = pb.createXor3(A, B, carry);
                carry = pb.createMajority3(pb.createNot(A), B, carry);
            }
            hash[y + j] = val;
        }
        #ifndef NDEBUG
        for (unsigned j = 0; j < m; ++j) {
            hash[x + j] = nullptr;
        }
        #endif
        select = pb.createAnd(select, pb.createAdvance(select, shft));
    }

    const auto f = ((steps & 1) == 0) ? 0 : m;

    Var * hashVar = getOutputStreamVar("hashes");
    for (unsigned i = 0; i < m; i++) {
        PabloAST * const expr = hash[f + i]; assert (expr);
     //   pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {expr});
        pb.createAssign(pb.createExtract(hashVar, pb.getInteger(i)), expr);
    }

  //  pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {run});


    // if the value is still in the select span, we did not include it in the hash'ed value.
    PabloAST * const selectors = pb.createAnd(run, pb.createNot(select), "selectors");

 //   pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {selectors});

  //  pb.createIntrinsicCall(pablo::Intrinsic::PrintRegister, {selectors});

    pb.createAssign(pb.createExtract(getOutputStreamVar("selector_span"), pb.getInteger(0)), selectors);

}

class MaskHash : public pablo::PabloKernel {
public:
    MaskHash(BuilderRef kb, kernel::StreamSet * hashes, kernel::StreamSet * mask, kernel::StreamSet * output);
protected:
    void generatePabloMethod() override;
};

MaskHash::MaskHash (BuilderRef b, kernel::StreamSet * hashes, kernel::StreamSet * mask, kernel::StreamSet * output)
: PabloKernel(b, "MashHash" + std::to_string(hashes->getNumElements()),
          {Binding{"hashes", hashes}, Binding{"mask", mask}},
          {Binding{"output", output}}) {

}


void MaskHash::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());
    PabloAST * mask = pb.createExtract(getInputStreamVar("mask"), pb.getInteger(0));
    const auto hashes = getInputStreamSet("hashes");
    for (unsigned i = 0; i < hashes.size(); ++i) {
        PabloAST * v = pb.createAnd(mask, hashes[i]);
        pb.createAssign(pb.createExtract(getOutputStreamVar("output"), pb.getInteger(i)), v);
    }
}


class NegateStreamSet : public pablo::PabloKernel {
public:
    NegateStreamSet(BuilderRef b, kernel::StreamSet * input, kernel::StreamSet * output);
protected:
    void generatePabloMethod() override;
};

NegateStreamSet::NegateStreamSet (BuilderRef b, kernel::StreamSet * input, kernel::StreamSet * output)
: PabloKernel(b, "NegateStreamSet" + std::to_string(input->getNumElements()),
          {Binding{"input", input}},
          {Binding{"output", output}}) {

}


void NegateStreamSet::generatePabloMethod() {
    PabloBuilder pb(getEntryScope());
    const auto input = getInputStreamSet("input");
    for (unsigned i = 0; i < input.size(); ++i) {
        PabloAST * v = pb.createNot(input[i]);
        pb.createAssign(pb.createExtract(getOutputStreamVar("output"), pb.getInteger(i)), v);
    }
}


class HashTable {
public:

    HashTable()
    : mTable(NumOfBuckets, 0)
    , mBitFlipHistory(NumOfBitFlips + 1, 0)
    , mHashCodeIndex(0)
    , mNumOfSameBitValues(0)
    , mNumOfDifferentBitValues(0) {

    }

    void insert(const uint64_t hash_code, const char * const /* start */, const char * const /* end */) {

        // errs() << hash_code << "\n";

        const uint64_t hb = NumOfHashBits;
        assert (hash_code < (1ULL << hb));
        const uint64_t nb = NumOfBuckets;
        const uint64_t idx = (hash_code * nb) >> hb;
        assert (idx < nb);
        // Since we know that the data read filters out any duplicates. We don't bother to
        // store the actual strings and instead just trust that they're unique.
        mTable[idx]++;
        // By construction of the input data, we know that we will see the original word then
        // NumOfBitFlips copies with exactly 1 bit flipped.

        mBitFlipHistory[mHashCodeIndex] = hash_code;

        if (mHashCodeIndex == NumOfBitFlips) {
            uint64_t diffCount = 0;
            for (unsigned i = 1; i <= NumOfBitFlips; ++i) {
                const auto A = mBitFlipHistory[i];
                for (unsigned j = 0; j < i; ++j) {
                    const auto B = mBitFlipHistory[j];
                    // if items differ, C's bit will be 1.
                    diffCount += __builtin_popcountll(A ^ B);
                }
            }

            mNumOfDifferentBitValues += diffCount;
            const size_t total = ((NumOfBitFlips * NumOfBitFlips) * NumOfHashBits) / 2;
            mNumOfSameBitValues += (total - diffCount);

            mHashCodeIndex = 0;
        } else {
            ++mHashCodeIndex;
        }



    }

    void clear() {
        for (auto & T : mTable) {
            T = 0;
        }
        for (auto & T : mBitFlipHistory) {
            T = 0;
        }
        mHashCodeIndex = 0;
        mNumOfSameBitValues = 0;
        mNumOfDifferentBitValues = 0;
    }

    double calculateScore() const {
        const auto p = percentDifference();
        assert (0.0 <= p && p <= 2.0);
        const auto c = chiSquareTest();
        errs() << "CHI: " << c << ", PD: " << p << "\n";
        return -c * std::pow(10.0, p);
    }

    double chiSquareTest() const {
        size_t totalCount = 0;
        for (const auto T : mTable) {
            totalCount += T;
        }
        if (LLVM_UNLIKELY(totalCount == 0)) {
            return std::numeric_limits<double>::quiet_NaN();
        }

        errs() << "SLOT,FREQ\n";

        const double expected = (double)(totalCount) / (double)(NumOfBuckets);
        double sum = 0.0;
        for (unsigned i = 0; i < mTable.size(); ++i) {
            const auto T = mTable[i];
            errs() << i << "," << T << "\n";
            const double x = (double)(T) - expected;
            // could optimize out but run the risk of the sum becoming too big
            sum += ((x * x) / expected);
        }
        errs() << "\n";
        return sum / (double)(NumOfBuckets);
    }

    double percentDifference() const {
        const size_t total = (mNumOfSameBitValues + mNumOfDifferentBitValues);
        if (LLVM_UNLIKELY(total == 0)) {
            return std::numeric_limits<double>::quiet_NaN();
        }
        size_t d = 0;
        if (mNumOfSameBitValues > mNumOfDifferentBitValues) {
            d = mNumOfSameBitValues - mNumOfDifferentBitValues;
        } else {
            d = mNumOfDifferentBitValues - mNumOfSameBitValues;
        }
        return ((double)2 * d) / ((double)total);
    }

private:
    std::vector<size_t>                 mTable;
    std::vector<uint64_t>               mBitFlipHistory;
    size_t                              mHashCodeIndex;

    size_t                              mNumOfSameBitValues;
    size_t                              mNumOfDifferentBitValues;

};


static HashTable HT;

typedef void (*HashDemoFunctionType)(const char * data, const size_t length);

extern "C" void hashtable_callback(uint64_t hashval, const char * const start, const char * const end) {
    HT.insert(hashval, start, end);
}

class PopulateHashTable : public MultiBlockKernel {
public:
    PopulateHashTable(BuilderRef b, StreamSet * const codeUnitStream, StreamSet * const SymbolEnds, StreamSet * const HashValues);

    void linkExternalMethods(BuilderRef b) override;
private:
    void generateMultiBlockLogic(BuilderRef b, llvm::Value * const numOfStrides) override;
};

PopulateHashTable::PopulateHashTable(BuilderRef b, StreamSet * const codeUnitStream, StreamSet * const SymbolEnds, StreamSet * const HashValues)
: MultiBlockKernel(b, "PopulateHashTable",
// inputs
{Binding{"codeUnitStream", codeUnitStream, FixedRate(), Deferred()} // <- not the right I/O def. should be the position of the symbol ends
, Binding{"SymbolEnds", SymbolEnds}
, Binding{"HashValues", HashValues, PopcountOf("SymbolEnds")}},
// outputs
{},
// input scalars
{},
// output scalars
{},
// kernel state
{}) {
     assert (SymbolEnds->getNumElements() == 1);
     assert (HashValues->getNumElements() == 1);
     assert (codeUnitStream->getNumElements() == 1);
     addAttribute(SideEffecting());
}

void PopulateHashTable::linkExternalMethods(BuilderRef b) {
    b->LinkFunction("hashtable_callback", hashtable_callback);
}

void PopulateHashTable::generateMultiBlockLogic(BuilderRef b, llvm::Value * const numOfStrides) {

    Constant * const sz_ZERO = b->getSize(0);
    Constant * const sz_ONE = b->getSize(1);
    IntegerType * sizeTy = b->getSizeTy();
    const auto sizeTyWidth = sizeTy->getBitWidth();

    Constant * const sz_BITS = b->getSize(sizeTyWidth);

    Constant * const sz_STRIDEWIDTH = b->getSize(mStride);

    assert (mStride == b->getBitBlockWidth());

    assert ((mStride % sizeTyWidth ) == 0);

    const auto vecsPerStride = mStride / sizeTyWidth;

    BasicBlock * const entryBlock = b->GetInsertBlock();

    // we expect that every block will have at least one marker

    BasicBlock * const stridePrologue = b->CreateBasicBlock("stridePrologue");
    BasicBlock * const strideCoordinateVecLoop = b->CreateBasicBlock("strideCoordinateVecLoop");
    BasicBlock * const strideCoordinateElemLoop = b->CreateBasicBlock("strideCoordinateElemLoop");
    BasicBlock * const strideCoordinateElemDone = b->CreateBasicBlock("strideCoordinateElemDone");
    BasicBlock * const strideCoordinateVecDone = b->CreateBasicBlock("strideCoordinateVecDone");
    BasicBlock * const strideCoordinatesDone = b->CreateBasicBlock("strideCoordinatesDone");

    Value * const initialProcessed = b->getProcessedItemCount("codeUnitStream");

    Value * const initialStrideCount = b->CreateUDiv(b->getProcessedItemCount("SymbolEnds"), sz_STRIDEWIDTH);

    Value * const hashProcessed = b->getProcessedItemCount("HashValues");

    b->CreateBr(stridePrologue);

    b->SetInsertPoint(stridePrologue);
    PHINode * const strideNumPhi = b->CreatePHI(sizeTy, 2);
    strideNumPhi->addIncoming(sz_ZERO, entryBlock);
    PHINode * const outerProcessedPhi = b->CreatePHI(sizeTy, 2);
    outerProcessedPhi->addIncoming(initialProcessed, entryBlock);
    PHINode * const outerHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    outerHashProcessedPhi->addIncoming(hashProcessed, entryBlock);

    Value * const symbolEnds = b->loadInputStreamBlock("SymbolEnds", sz_ZERO, strideNumPhi);
    Value * const strideBaseCharacterOffset = b->CreateMul(b->CreateAdd(initialStrideCount, strideNumPhi), sz_STRIDEWIDTH);

    FixedVectorType * const sizeVecTy = FixedVectorType::get(sizeTy, vecsPerStride);
    Value * const symbolEndVec = b->CreateBitCast(symbolEnds, sizeVecTy);

    b->CreateLikelyCondBr(b->bitblock_any(symbolEnds), strideCoordinateVecLoop, strideCoordinateVecDone);

    b->SetInsertPoint(strideCoordinateVecLoop);
    PHINode * const elemIdx = b->CreatePHI(sizeTy, 2, "elemIdx");
    elemIdx->addIncoming(sz_ZERO, stridePrologue);
    PHINode * const incomingProcessedPhi = b->CreatePHI(sizeTy, 2);
    incomingProcessedPhi->addIncoming(outerProcessedPhi, stridePrologue);
    PHINode * const incomingHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    incomingHashProcessedPhi->addIncoming(outerHashProcessedPhi, stridePrologue);

    Value * const elem = b->CreateExtractElement(symbolEndVec, elemIdx);
    b->CreateCondBr(b->CreateICmpNE(elem, sz_ZERO), strideCoordinateElemLoop, strideCoordinateElemDone);

    b->SetInsertPoint(strideCoordinateElemLoop);
    PHINode * const remaining = b->CreatePHI(sizeTy, 2);
    remaining->addIncoming(elem, strideCoordinateVecLoop);
    PHINode * const innerProcessedPhi = b->CreatePHI(sizeTy, 2);
    innerProcessedPhi->addIncoming(incomingProcessedPhi, strideCoordinateVecLoop);
    PHINode * const innerHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    innerHashProcessedPhi->addIncoming(incomingHashProcessedPhi, strideCoordinateVecLoop);

    Value * pos = b->CreateCountForwardZeroes(remaining);
    pos = b->CreateAdd(pos, b->CreateMul(elemIdx, sz_BITS));
    pos = b->CreateAdd(pos, strideBaseCharacterOffset);

    FixedArray<Value *, 3> args;
    Value * const hashPtr = b->getRawInputPointer("HashValues", innerHashProcessedPhi);

    args[0] = b->CreateZExt(b->CreateLoad(hashPtr), sizeTy);
    args[1] = b->getRawInputPointer("codeUnitStream", innerProcessedPhi);
    args[2] = b->getRawInputPointer("codeUnitStream", pos);

    Function * callbackFn = b->getModule()->getFunction("hashtable_callback"); assert (callbackFn);
    b->CreateCall(callbackFn->getFunctionType(), callbackFn, args);

    Value * const nextRemaining = b->CreateResetLowestBit(remaining);

    remaining->addIncoming(nextRemaining, strideCoordinateElemLoop);

    Value * const nextHashProcessed = b->CreateAdd(innerHashProcessedPhi, sz_ONE);

    innerHashProcessedPhi->addIncoming(nextHashProcessed, strideCoordinateElemLoop);
    innerProcessedPhi->addIncoming(pos, strideCoordinateElemLoop);

    b->CreateCondBr(b->CreateICmpNE(nextRemaining, sz_ZERO), strideCoordinateElemLoop, strideCoordinateElemDone);

    b->SetInsertPoint(strideCoordinateElemDone);

    PHINode * const nextProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextProcessedPhi->addIncoming(incomingProcessedPhi, strideCoordinateVecLoop);
    nextProcessedPhi->addIncoming(pos, strideCoordinateElemLoop);
    incomingProcessedPhi->addIncoming(nextProcessedPhi, strideCoordinateElemDone);

    PHINode * const nextHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextHashProcessedPhi->addIncoming(outerHashProcessedPhi, strideCoordinateVecLoop);
    nextHashProcessedPhi->addIncoming(nextHashProcessed, strideCoordinateElemLoop);
    incomingHashProcessedPhi->addIncoming(nextHashProcessedPhi, strideCoordinateElemDone);

    Value * const nextElemIdx = b->CreateAdd(elemIdx, sz_ONE);
    elemIdx->addIncoming(nextElemIdx, strideCoordinateElemDone);
    Value * const moreVecs = b->CreateICmpNE(nextElemIdx, b->getSize(vecsPerStride));
    b->CreateCondBr(moreVecs, strideCoordinateVecLoop, strideCoordinateVecDone);

    b->SetInsertPoint(strideCoordinateVecDone);
    PHINode * const nextOuterProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextOuterProcessedPhi->addIncoming(nextProcessedPhi, strideCoordinateElemDone);
    nextOuterProcessedPhi->addIncoming(outerProcessedPhi, stridePrologue);
    outerProcessedPhi->addIncoming(nextOuterProcessedPhi, strideCoordinateVecDone);

    PHINode * const nextOuterHashProcessedPhi = b->CreatePHI(sizeTy, 2);
    nextOuterHashProcessedPhi->addIncoming(nextHashProcessedPhi, strideCoordinateElemDone);
    nextOuterHashProcessedPhi->addIncoming(outerHashProcessedPhi, stridePrologue);
    outerHashProcessedPhi->addIncoming(nextOuterHashProcessedPhi, strideCoordinateVecDone);

    Value * const nextStrideNum = b->CreateAdd(strideNumPhi, sz_ONE);
    strideNumPhi->addIncoming(nextStrideNum, strideCoordinateVecDone);

    b->CreateCondBr(b->CreateICmpULT(nextStrideNum, numOfStrides), stridePrologue, strideCoordinatesDone);

    b->SetInsertPoint(strideCoordinatesDone);
    b->setProcessedItemCount("codeUnitStream", nextOuterProcessedPhi);
}

#ifndef USE_BIX_SUB_HASH
HashDemoFunctionType hashdemo_gen (CPUDriver & driver, const BixHashGenome & genome) {
#else
HashDemoFunctionType hashdemo_gen (CPUDriver & driver) {
#endif
    auto & b = driver.getBuilder();
    auto P = driver.makePipeline({Binding{b->getInt8PtrTy(), "input"}, Binding{b->getSizeTy(), "length"}}, {});

    std::string tmp;
    raw_string_ostream nm(tmp);
    nm << "hashtester" << NumOfBasisBits << ":" << NumOfHashBits;
    P->setUniqueName(nm.str());

    Scalar * input = P->getInputScalar("input");
    Scalar * length = P->getInputScalar("length");

    // Source data
    StreamSet * const codeUnitStream = P->CreateStreamSet(1, NumOfBasisBits);
    P->CreateKernelCall<MemorySourceKernel>(input, length, codeUnitStream);

    StreamSet * const u8basis = P->CreateStreamSet(NumOfBasisBits);
    P->CreateKernelCall<S2PKernel>(codeUnitStream, u8basis);

    StreamSet * LineFeeds = P->CreateStreamSet(1);
    P->CreateKernelCall<LineFeedKernelBuilder>(u8basis, LineFeeds);

    StreamSet * Runs = P->CreateStreamSet(1);
    P->CreateKernelCall<NegateStreamSet>(LineFeeds, Runs);

    StreamSet * const BixHashes = P->CreateStreamSet(NumOfHashBits);
    StreamSet * SelectorSpans = P->CreateStreamSet(1);
#ifndef USE_BIX_SUB_HASH
    P->CreateKernelFamilyCall<BixHashGenerator>(genome, u8basis, Runs, BixHashes, SelectorSpans);
#else
    P->CreateKernelFamilyCall<BixSubHash>(u8basis, Runs, BixHashes, SelectorSpans, 4, BaseSeed);
#endif

    StreamSet * SymbolEnds = P->CreateStreamSet(1);
    P->CreateKernelCall<IdentifyLastSelector>(SelectorSpans, SymbolEnds);

    StreamSet * const compressed = P->CreateStreamSet(NumOfHashBits);

    P->CreateKernelCall<FieldCompressKernel>(Select(SymbolEnds, {0}), SelectOperationList{Select(BixHashes, streamutils::Range(0, NumOfHashBits))}, compressed, 64);

    StreamSet * const Hashes = P->CreateStreamSet(NumOfHashBits);

    P->CreateKernelCall<StreamCompressKernel>(SymbolEnds, compressed, Hashes, 64);

    StreamSet * masked = P->CreateStreamSet(NumOfHashBits);
    P->CreateKernelCall<MaskHash>(BixHashes, SymbolEnds, masked);

    StreamSet * const HashValues = P->CreateStreamSet(1, 1UL << ceil_log2(NumOfHashBits));

    if (NumOfHashBits <= 8) {
        P->CreateKernelCall<P2SKernel>(Hashes, HashValues);
    } else if (NumOfHashBits <= 16) {
        P->CreateKernelCall<P2S16Kernel>(Hashes, HashValues);
    } else {
        P->CreateKernelCall<P2S32Kernel>(Hashes, HashValues);
    }

    P->CreateKernelCall<PopulateHashTable>(codeUnitStream, SymbolEnds, HashValues);

    auto f = reinterpret_cast<HashDemoFunctionType>(P->compile());
    assert (f);
    return f;
}

class bixhash_optimization_problem {
public:
    using individual_type = std::shared_ptr<BixHashGenome>;
    using generator_type = std::mt19937;
    using fitness_type = double;


    constexpr static double recombination_rate = 0.4;

    constexpr static double mutation_rate = 0.02;

    auto evaluate(const individual_type & x, generator_type&) const -> fitness_type {
#ifndef USE_BIX_SUB_HASH
        CPUDriver pxDriver("hashtester");
        auto func = hashdemo_gen(pxDriver, *x);
        HT.clear();
        func(InputData.data(), InputData.size());
#endif
        const auto r = HT.calculateScore();
        return r;
    }

    inline static auto nextLayerStart(const size_t u) -> size_t {
        if (u < NumOfBasisBits) {
            return NumOfHashBits;
        } else {
            assert (u < (NumOfBasisBits + (NumOfHashBits * NumOfSteps)));
            const auto m = (u - NumOfBasisBits) / NumOfHashBits;
            return (m + 1) * (NumOfHashBits) + NumOfBasisBits;
        }
    };

    inline static auto currentLayerStart(const size_t v) -> size_t {
        assert (v >= NumOfBasisBits);
        const auto m = (v - NumOfBasisBits) / NumOfHashBits;
        return m * (NumOfHashBits) + NumOfBasisBits;
    };

    auto recombine(const individual_type & x, const individual_type & y,
                  generator_type& rng) const -> std::array<individual_type, 2u> {

        if (!ga::draw(recombination_rate, rng)) {
            return std::array<individual_type, 2u> {x, y};
        }

#if 1

        const BixHashGenome & A = *x;
        const BixHashGenome & B = *y;

        auto C = std::make_shared<BixHashGenome>();
        auto D = std::make_shared<BixHashGenome>();

        C->BasisSelectors.resize(NumOfHashBits);
        D->BasisSelectors.resize(NumOfHashBits);

        for (unsigned i = 0; i < NumOfHashBits; ++i) {
            C->BasisSelectors[i].resize(NumOfBasisBits, false);
            D->BasisSelectors[i].resize(NumOfBasisBits, false);
        }

        std::uniform_int_distribution<size_t> pos(1, NumOfHashBits * NumOfBasisBits - 2);
        const auto k = pos(rng);

        const auto toCopy = (k / NumOfHashBits);

        for (unsigned i = 0; i < toCopy; ++i) {
            C->BasisSelectors[i] = A.BasisSelectors[i];
            D->BasisSelectors[i] = B.BasisSelectors[i];
        }

        const auto p = k - (toCopy * NumOfHashBits);

        const auto & a = A.BasisSelectors[toCopy];
        const auto & b = B.BasisSelectors[toCopy];

        unsigned ai = a.find_first();
        while (ai < p) {
            assert (ai != -1U);
            C->BasisSelectors[toCopy].set(ai);
            ai = a.find_next(ai);
        }
        while (ai != -1U) {
            D->BasisSelectors[toCopy].set(ai);
            ai = a.find_next(ai);
        }

        unsigned bi = b.find_first();
        while (bi < p) {
            assert (bi != -1U);
            D->BasisSelectors[toCopy].set(bi);
            bi = b.find_next(bi);
        }
        while (bi != -1U) {
            C->BasisSelectors[toCopy].set(bi);
            bi = b.find_next(bi);
        }

        for (unsigned i = toCopy + 1; i < NumOfHashBits; ++i) {
            C->BasisSelectors[i] = B.BasisSelectors[i];
            D->BasisSelectors[i] = A.BasisSelectors[i];
        }

        std::uniform_int_distribution<size_t> pos2(1, NumOfHashBits * NumOfSteps - 2);
        const auto k2 = pos2(rng);

        const auto toCopy2 = (k2 / NumOfHashBits);

        assert (toCopy2 < NumOfSteps);

        C->MixOrderings.resize(NumOfSteps);
        D->MixOrderings.resize(NumOfSteps);

        for (unsigned i = 0; i < toCopy2; ++i) {
            C->MixOrderings[i] = A.MixOrderings[i];
            D->MixOrderings[i] = B.MixOrderings[i];
        }

        const auto p2 = k2 - (toCopy2 * NumOfHashBits);

        BitVector observed(NumOfHashBits, false);

        auto copy_selected = [&](const std::vector<unsigned> & A, const std::vector<unsigned> & B, std::vector<unsigned> & output) {

            assert (output.size() == 0);

            output.reserve(NumOfHashBits);

            for (unsigned i = 0; i < p; ++i) {
                const auto a = A[i];
                observed.set(a);
                output.push_back(a);
            }

            for (unsigned i = 0; i < NumOfHashBits; ++i) {
                const auto b = B[i];
                if (!observed.test(b)) {
                    output.push_back(b);
                }
            }

            assert (output.size() == NumOfHashBits);

        };

        copy_selected(A.MixOrderings[toCopy2], B.MixOrderings[toCopy2], C->MixOrderings[toCopy2]);

        observed.reset();

        copy_selected(B.MixOrderings[toCopy2], A.MixOrderings[toCopy2], D->MixOrderings[toCopy2]);

        for (unsigned i = toCopy2 + 1; i < NumOfSteps; ++i) {
            C->MixOrderings[i] = A.MixOrderings[i];
            D->MixOrderings[i] = B.MixOrderings[i];
        }

        return std::array<individual_type, 2u> {std::move(C), std::move(D)};
#else

        const auto size = NumOfBasisBits + NumOfHashBits * (NumOfSteps + 1);

        const BixHashGenome & A = *x;
        const BixHashGenome & B = *y;

        assert (num_vertices(A) == num_vertices(B));
        assert (num_vertices(A) == size);

        BixHashGenome::edge_iterator ea, ea_end;
        std::tie(ea, ea_end) = edges(A);
        BixHashGenome::edge_iterator eb, eb_end;
        std::tie(eb, eb_end) = edges(B);

        std::uniform_int_distribution<size_t> pos(1, size - 2);

        const auto p = pos(rng);

        auto C = std::make_shared<BixHashGenome>(size);
        auto D = std::make_shared<BixHashGenome>(size);

        for (size_t i = 0; i < p; ++i) {
            for (auto e : make_iterator_range(in_edges(i, A))) {
                add_edge(source(e, A), target(e, A), *C);
            }
            for (auto e : make_iterator_range(in_edges(i, B))) {
                add_edge(source(e, B), target(e, B), *D);
            }
        }

        for (size_t i = p + 1; i < size; ++i) {
            for (auto e : make_iterator_range(in_edges(i, A))) {
                add_edge(source(e, A), target(e, A), *D);
            }
            for (auto e : make_iterator_range(in_edges(i, B))) {
                add_edge(source(e, B), target(e, B), *C);
            }
        }

        for (size_t i = size - NumOfHashBits; i-- >= NumOfBasisBits; ) {
            if (out_degree(i, *C) == 0) {
                clear_in_edges(i, *C);
            }
        }

        for (size_t i = size - NumOfHashBits; i-- >= NumOfBasisBits; ) {
            if (out_degree(i, *D) == 0) {
                clear_in_edges(i, *D);
            }
        }

        auto fixChild = [&](BixHashGenome & X) {
            for (size_t u = NumOfBasisBits; u < size; ++u) {
                if (in_degree(u, X) == 0) {
                    std::uniform_int_distribution<size_t> W(0, currentLayerStart(u) - 1);
                    const auto w = W(rng);
                    assert (w < u);
                    add_edge(w, u, X);
                }
            }

            for (size_t u = NumOfBasisBits; u < size - NumOfHashBits; ++u) {
                if (out_degree(u, X) == 0) {
                    const auto s = nextLayerStart(u);
                    assert (s < size);
                    std::uniform_int_distribution<size_t> V(s, size - 1);
                    const auto v = V(rng);
                    assert (u < v);
                    assert (v < size);
                    add_edge(u, v, X);
                }
            }
        };

        fixChild(*C);
        fixChild(*D);

        return std::array<individual_type, 2u> {std::move(C), std::move(D)};
#endif
    }

    auto mutate(individual_type & x, generator_type& g) const -> void {

        auto & B = x->BasisSelectors;

        std::binomial_distribution<size_t> H(NumOfBasisBits, 1.0 / (double)(NumOfBasisBits));

        std::uniform_int_distribution<size_t> U(0, NumOfBasisBits - 1);

        for (unsigned i = 0; i < NumOfHashBits; ++i) {
            auto & bb = B[i];
            const auto k = H(g);
            for (unsigned j = 0; j < k; ++j) {
                bb.flip(U(g));
            }
        }

        auto & M = x->MixOrderings;

        std::binomial_distribution<size_t> S(NumOfHashBits / 2, 1.0 / (double)(NumOfHashBits));

        std::uniform_int_distribution<size_t> V(0, NumOfHashBits - 1);

        for (unsigned i = 0; i < NumOfSteps; ++i) {
            auto & mm = M[i];
            const auto k = S(g);
            for (unsigned j = 0; j < k; ++j) {
                const auto a = V(g);
                const auto b = V(g);
                if (a != b) {
                    std::swap(mm[a], mm[b]);
                }
            }
        }



#if 0
        // we want to flip one of the edge/non-edges

        BixHashGenome & C = *x;

        const auto n = num_vertices(C);
        assert (n == (NumOfBasisBits + NumOfHashBits * (NumOfSteps + 1)));

        std::binomial_distribution<size_t> P(n, 0.2);

        const auto count = P(g);

        for (unsigned i = 0; i < count; ++i) {

            std::uniform_int_distribution<size_t> U(0, n - (NumOfHashBits + 1));
            const auto u = U(g);
            const auto s = nextLayerStart(u);
            assert (s > u);
            std::uniform_int_distribution<size_t> V(s, n - 1);
            const auto v = V(g);
            assert (v >= NumOfBasisBits);
            assert (u < v);

            if (edge(u, v, C).second) {
                remove_edge(u, v, C);
                if (out_degree(u, C) == 0) {
                    std::uniform_int_distribution<size_t> V(s, n - 1);
                    const auto w = V(g);
                    assert (u < w);
                    add_edge(u, w, C);
                }
                if (in_degree(v, C) == 0) {
                    std::uniform_int_distribution<size_t> U(0, currentLayerStart(v) - 1);
                    const auto w = U(g);
                    assert (w < v);
                    add_edge(w, v, C);
                }
            } else {
                add_edge(u, v, C);
            }

        }
#endif
    }

    bixhash_optimization_problem(std::vector<char> && data)
    : InputData(std::move(data)) {

    }

    static std::vector<individual_type> generate(const size_t count, generator_type & rng) {

        constexpr size_t n = NumOfBasisBits;
        const size_t m = NumOfHashBits;
        const size_t steps = NumOfSteps;

        const auto size = n + m * steps;

        std::vector<individual_type> P;
        P.reserve(count);

        std::vector<unsigned> selected(m);
        std::iota(selected.begin(), selected.end(), 0);

        for (unsigned p = 0; p < count; ++p) {

            auto C = std::make_shared<BixHashGenome>();

            auto & variables = C->BasisSelectors;

            variables.resize(m);

            for (unsigned i = 0; i < m; ++i) {
                assert (variables[i].size() == 0);
                variables[i].resize(n, false);
                assert (variables[i].empty());
            }

            std::binomial_distribution<size_t> M(m, 0.25);

            auto selected_pos = selected.end();



            for (unsigned i = 0; i < n; ++i) {
                if (selected_pos == selected.end()) {
                    std::shuffle(selected.begin(), selected.end(), rng);
                    selected_pos = selected.begin();
                }
                const auto k = M(rng) + 1U;
                for (unsigned j = 0; j != k; ++j) {
                    variables[*selected_pos++].set(i);
                }
            }

            auto & orderings = C->MixOrderings;

            orderings.resize(steps);

            for (unsigned i = 0; i < steps; ++i) {
                auto & O = orderings[i];
                O.resize(m);
                std::iota(O.begin(), O.end(), 0);
                std::shuffle(O.begin(), O.end(), rng);
            }

#if 0

            auto & variables = *C;

            for (unsigned i = 0; i < m; ++i) {
                assert (variables[i].size() == 0);
                variables[i].resize(size, false);
            }

            std::binomial_distribution<size_t> M(3, 0.2);

            auto selected_pos = selected.end();

            for (unsigned i = 0; i < size; ++i) {
                if (selected_pos == selected.end()) {
                    std::shuffle(selected.begin(), selected.end(), rng);
                    selected_pos = selected.begin();
                }
                const auto k = M(rng) + 1U;
                for (unsigned j = 0; j != k; ++j) {
                    variables[*selected_pos++].set(i);
                }
            }

#endif

#if 0

            std::binomial_distribution<size_t> M(4, 0.3);

            for (size_t step = 0, begin = 0; step < steps; ++step) {

                const auto s = n + m * step;
                assert (begin < s);
                const auto e = s + m;

                auto target_pos = targets.end();

                for (size_t u = begin; u < s; ++u) {

                    size_t m = (M(rng) * 2) + 1;
                    while (m) {

                        if (target_pos == targets.end()) {
                            std::shuffle(targets.begin(), targets.end(), rng);
                            target_pos = targets.begin();
                        }

                        const size_t v = s + (*target_pos);
                        ++target_pos;
                        if (edge(u, v, *C).second) {
                            remove_edge(u, v, *C);
                            ++m;
                        } else {
                            add_edge(u, v, *C);
                            --m;
                        }
                    }
                }
                begin = s;
            }
#endif

#if 0

            std::binomial_distribution<size_t> M(4, 0.3);

            for (size_t step = 0, begin = 0; step < steps; ++step) {

                const auto s = n + m * step;
                assert (begin < s);
                const auto e = s + m;

                auto target_pos = targets.end();

                for (size_t u = begin; u < s; ++u) {

                    size_t m = (M(rng) * 2) + 1;
                    while (m) {

                        if (target_pos == targets.end()) {
                            std::shuffle(targets.begin(), targets.end(), rng);
                            target_pos = targets.begin();
                        }

                        const size_t v = s + (*target_pos);
                        ++target_pos;
                        if (edge(u, v, *C).second) {
                            remove_edge(u, v, *C);
                            ++m;
                        } else {
                            add_edge(u, v, *C);
                            --m;
                        }
                    }
                }
                begin = s;
            }
#endif

#if 0
            for (size_t step = 0; step < steps; ++step) {

                const auto s = n + m * step;
                const auto e = s + m;

                const auto l = (step == 0) ? n : m;

                std::uniform_int_distribution<size_t> U(s - l, s - 1);

                std::uniform_int_distribution<size_t> W(e, e + m - 1);

                std::binomial_distribution<size_t> M(3, 0.5);

                for (size_t v = s; v < e; ++v) {
                    for (size_t m = M(rng) + 1ULL; m; --m) {
                        const size_t u = U(rng);
                        if (!edge(u, v, *C).second) {
                            add_edge(u, v, *C);
                        }
                    }
                }

                if (step < (steps - 1)) {

                    for (size_t v = s; v < e; ++v) {
                        for (size_t m = M(rng) + 1ULL; m; --m) {
                            const size_t w = W(rng);
                            if (!edge(v, w, *C).second) {
                                add_edge(v, w, *C);
                            }
                        }
                    }

                }
            }

//            const auto s = n + m * steps;
//            const auto e = size;
//            assert ((s + m) == size);

//            std::uniform_int_distribution<size_t> U(0, s - 1);

//            std::binomial_distribution<size_t> M(m - 1, 0.2);

//            for (size_t v = s; v < e; ++v) {
//                for (size_t m = M(rng) + 1ULL; m; --m) {
//                    const size_t u = U(rng);
//                    if (!edge(u, v, *C).second) {
//                        add_edge(u, v, *C);
//                    }
//                }
//            }
#endif

   //         printGraph(*C, errs(), "C");

            P.emplace_back(std::move(C));
        }

        assert (P.size() == count);

        return P;
    }

#if 0

private:

    static void clean_genome(BixHashGraphGenome & G) {
        // We have N basis bits and STEPS * (M + 1) other verticies. Each layer of M vertices defines what will eventually
        // be written. If a layer is not the last, it must have an out degree of at least one or it will simply generate
        // dead code.
        const auto limit = NumOfBasisBits + NumOfSteps * (NumOfHashBits + 1);
        assert (num_vertices(G) == limit);
        for (unsigned i = limit; i >= NumOfBasisBits; --i) {
            if (out_degree(i, G) == 0) {
                clear_in_edges(i, G);
            }
        }
    }

#endif

private:

    const std::vector<char> InputData;

};

std::vector<char> readFileData(const std::vector<fs::path> & fileNames, bixhash_optimization_problem::generator_type & rng) {


    char * line = nullptr;
    size_t len = 0;
    const ssize_t maxLineSize = 1UL << NumOfSteps;

    size_t requiredSize = 0;

    for (const fs::path & fileName: fileNames) {
        struct stat st;
        if (LLVM_LIKELY(stat(fileName.c_str(), &st) == 0)) {
            requiredSize += st.st_size;
        } else {
            report_fatal_error("cannot determine size of " + fileName.string());
        }
    }

    std::vector<char> data(requiredSize * (NumOfBitFlips + 1));

    BitVector T(maxLineSize * 8);

    DenseSet<StringRef> observed;

    size_t current = 0;


    for (const fs::path & fileName: fileNames) {

        FILE * const fp = fopen(fileName.c_str(), "r");
        if (LLVM_UNLIKELY(fp == nullptr)) {
            report_fatal_error("cannot open " + fileName.string() + ".");
        }

        for (;;) {
            const auto r = getline(&line, &len, fp);
            // r is -1 or the number of characters read, including the delimiter
            if (LLVM_UNLIKELY(r == -1)) {
                break;
            }
            if (LLVM_UNLIKELY(r == 0)) {
                continue;
            }
            const auto m = std::min<size_t>(r, maxLineSize);

            // To simplify the process, we just discard any entries that aren't long
            // enough for bit flipping
            if (NumOfBitFlips > (m * 8)) {
                continue;
            }

            // we want only unique keys in the list to avoid needing sub-hashtables
            // for unique keys later.
            auto f = observed.find(StringRef{line, m});
            if (LLVM_UNLIKELY(f != observed.end())) {
                continue;
            }

            assert (data.size() >= (current + (m + 1) * (NumOfBitFlips + 1)));

            std::memcpy(&data[current], line, m);

            observed.insert(StringRef{&data[current], m});

            auto p = current + m;

            data[p++] = '\n';

            T.reset();

            // generate bit flipped data from the original "word"
            for (unsigned i = 0; i < NumOfBitFlips; ++i) {

                std::memcpy(&data[p], line, m);

                assert (!T.all());

                assert (i <= (m * 8) - 1);

                std::uniform_int_distribution<size_t> pos(0, (m * 8) - 1 - i);

                const auto j = pos(rng);

                for (auto k = T.find_next_unset(j); k != -1; k = T.find_next_unset(k)) {

                    char & c = data[p + (k / 8)];

                    c ^= (1U << (k & 7U));

                    T.set(k);
                }

                p += m;

                data[p++] = '\n';
            }

            current = p;
        }

        fclose(fp);
    }
    free(line);
    return data;
}

void runMurmur3HashOnData(const std::vector<char> & data) {

     HT.clear();

    size_t start = 0;
    const auto len = data.size();

    const size_t mask = (1ULL << ((size_t)NumOfHashBits)) - 1ULL;

    for (size_t i = 0; i < len; ++i) {
        if (data[i] == '\n') {
            if (start + 1 < i) {
                uint32_t hashOut = 0;
                MurmurHash3_x86_32(&data[start], i - start, 0, &hashOut);
                HT.insert(hashOut & mask, &data[start], &data[i]);
            }
            start = i + 1;
        }
    }

    errs() << "MURMUR3 SCORE: CHI=" << HT.chiSquareTest() << " % DIFF=" << HT.percentDifference() << "\n";


}

void runSubHashHashOnData(const std::vector<char> & data) {
#ifdef USE_BIX_SUB_HASH
    CPUDriver pxDriver("hashtester");
    auto func = hashdemo_gen(pxDriver);
    HT.clear();
    func(data.data(), data.size());
    errs() << "BIXHASH SCORE: CHI=" << HT.chiSquareTest() << " % DIFF=" << HT.percentDifference() << "\n";
#endif
}

int main(int argc, char *argv[]) {
    codegen::ParseCommandLineOptions(argc, argv, {&HashDemoOptions, pablo_toolchain_flags(), codegen::codegen_flags()});

    CPUDriver pxDriver("hashtester");


    auto allFiles = argv::getFullFileList(pxDriver, inputFiles);


    uint64_t seed = 0;
    if (BaseSeed.isDefaultOption()) {
        std::random_device rng;
        seed = rng();
    } else {
        seed = BaseSeed;
    }

    bixhash_optimization_problem::generator_type generator(seed);

    auto fileData = readFileData(allFiles, generator);

    runMurmur3HashOnData(fileData);

    runSubHashHashOnData(fileData);

    #ifndef USE_BIX_SUB_HASH

    if (Murmur3Only) {
        return 0;
    }

    bixhash_optimization_problem P(std::move(fileData));

    std::size_t elite_count = 0;
    const unsigned generation_count = 1;
    const unsigned population_count = 1;



    errs() << "INIT POP:\n";

    auto initial_population = bixhash_optimization_problem::generate(population_count, generator);

    errs() << "INIT ALGO:\n";

     HT.clear();

    ga::algorithm<bixhash_optimization_problem> algorithm(std::move(P), std::move(initial_population), elite_count, std::move(generator));

    // ===== Iterate the algorithm ===== //

    for (unsigned i = 0u; i < generation_count; ++i) {
        errs() << "ROUND: " << i << "\n";
        algorithm.iterate();
        const auto & solution = algorithm.population().front();
        errs() << " -- current fitness: " << solution.fitness << "\n";
    }
    // ===== Retrieve solution information ===== //

    // Every candidate solution in the population sorted by fitness.
    auto & solution = algorithm.population().front();

    errs() << BixHashGenerator::makeGenName(*solution.x);

   // printGraph(*(solution.x), errs(), "SOLUTION");
    errs() << "\n\nFITNESS: " << solution.fitness << "\n";
    #endif

    return 0;
}
